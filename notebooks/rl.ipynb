{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Scratch Deep Q Reinforcement Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a simpler library called keras-rl that adds some easy plug & play setup for rl models, however, this sample sets up one manually for the mountain car openai gym example.\n",
    "\n",
    "Summary of DQN - Reinforcement learning is an example of deep learning applied - not a different type of model.  Refinforcement learning can be applied with/without neural networks.  Refinforcement learning is defined by two entities - an environment and an agent.  The agent is the bot or automation that takes action upon the environment to reach an accept state within the environment.  Essentially a pattern is followed - for each timestep in a given trial/session, take an action.  This action will either be random, or can come from memory (epsilon - exploration vs. exploitation).  After the action, record and memorize the state of the environment.  Then using samples from memory, train on how to make better future predicitions.  The prediction network uses a multi-layer perceptron model to make better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8BisKmezg4D6"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6ec83f66523e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGM4cF91973C"
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, env):\n",
    "        self.env     = env\n",
    "        self.memory  = deque(maxlen=2000)\n",
    "        \n",
    "        self.gamma = 0.85\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.005\n",
    "        self.tau = .125\n",
    "\n",
    "        self.model        = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        model   = Sequential()\n",
    "        state_shape  = self.env.observation_space.shape\n",
    "        model.add(Dense(24, input_dim=state_shape[0], activation=\"relu\"))\n",
    "        model.add(Dense(48, activation=\"relu\"))\n",
    "        model.add(Dense(24, activation=\"relu\"))\n",
    "        model.add(Dense(self.env.action_space.n))\n",
    "        model.compile(loss=\"mean_squared_error\",\n",
    "            optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.append([state, action, reward, new_state, done])\n",
    "\n",
    "    def replay(self):\n",
    "        batch_size = 32\n",
    "        if len(self.memory) < batch_size: \n",
    "            return\n",
    "\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        for sample in samples:\n",
    "            state, action, reward, new_state, done = sample\n",
    "            target = self.target_model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                Q_future = max(self.target_model.predict(new_state)[0])\n",
    "                target[0][action] = reward + Q_future * self.gamma\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "    def target_train(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = weights[i] * self.tau + target_weights[i] * (1 - self.tau)\n",
    "        self.target_model.set_weights(target_weights)\n",
    "\n",
    "    def save_model(self, fn):\n",
    "        self.model.save(fn)\n",
    "\n",
    "def main():\n",
    "    env     = gym.make(\"MountainCar-v0\")\n",
    "    gamma   = 0.9\n",
    "    epsilon = .95\n",
    "\n",
    "    trials  = 1000\n",
    "    trial_len = 500\n",
    "\n",
    "    # updateTargetNetwork = 1000\n",
    "    dqn_agent = DQN(env=env)\n",
    "    steps = []\n",
    "    for trial in range(trials):\n",
    "        cur_state = env.reset().reshape(1,2)\n",
    "        for step in range(trial_len):\n",
    "            action = dqn_agent.act(cur_state)\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # reward = reward if not done else -20\n",
    "            new_state = new_state.reshape(1,2)\n",
    "            dqn_agent.remember(cur_state, action, reward, new_state, done)\n",
    "            \n",
    "            dqn_agent.replay()       # internally iterates default (prediction) model\n",
    "            dqn_agent.target_train() # iterates target model\n",
    "\n",
    "            cur_state = new_state\n",
    "            if done:\n",
    "                break\n",
    "        if step >= 199:\n",
    "            print(\"Failed to complete in trial {}\".format(trial))\n",
    "            if step % 10 == 0:\n",
    "                dqn_agent.save_model(\"trial-{}.model\".format(trial))\n",
    "        else:\n",
    "            print(\"Completed in {} trials\".format(trial))\n",
    "            dqn_agent.save_model(\"success.model\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "Oq4fKhGC-Bv3",
    "outputId": "90df662a-8007-4506-b257-b3c8c4e5a5d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to complete in trial 0\n",
      "Failed to complete in trial 1\n",
      "Failed to complete in trial 2\n",
      "Failed to complete in trial 3\n",
      "Failed to complete in trial 4\n",
      "Failed to complete in trial 5\n",
      "Failed to complete in trial 6\n",
      "Failed to complete in trial 7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-d8d522d0fb68>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mdqn_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mdqn_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# internally iterates default (prediction) model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mdqn_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# iterates target model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-d8d522d0fb68>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mQ_future\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mQ_future\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1170\u001b[0m           self._maybe_load_initial_epoch_from_ckpt(initial_epoch))\n\u001b[1;32m   1171\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1172\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1173\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    694\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    717\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 719\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m       \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3119\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3120\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 3121\u001b[0;31m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[1;32m   3122\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3123\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mm7QMME0-Cw9"
   },
   "outputs": [],
   "source": [
    "env     = gym.make(\"MountainCar-v0\")\n",
    "cur_state = env.reset().reshape(1,2)\n",
    "steps = []\n",
    "gamma   = 0.9\n",
    "epsilon = .95\n",
    "trials  = 1000\n",
    "trial_len = 500\n",
    "dqn_agent = DQN(env=env)\n",
    "for step in range(trial_len):\n",
    "    action = dqn_agent.act(cur_state)\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    new_state = new_state.reshape(1,2)\n",
    "    cur_state = new_state\n",
    "    if done:\n",
    "        break\n",
    "if step >= 199:\n",
    "    print('Failed to complete trial')\n",
    "    if step % 10 == 0:\n",
    "        dqn_agent.save_model(\"trial-{}.model\".format(trial))\n",
    "else:\n",
    "    print(\"Completed in Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7YymEehF3-ga",
    "outputId": "be041e5e-62c3-47eb-94bc-b4f96ec62466"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1000 finished after 200 episode steps with total reward = -3037.000000.\n",
      "Episode 2/1000 finished after 200 episode steps with total reward = -3016.000000.\n",
      "Episode 3/1000 finished after 200 episode steps with total reward = -3100.000000.\n",
      "Episode 4/1000 finished after 200 episode steps with total reward = -3163.000000.\n",
      "Episode 5/1000 finished after 200 episode steps with total reward = -2932.000000.\n",
      "Episode 6/1000 finished after 200 episode steps with total reward = -3058.000000.\n",
      "Episode 7/1000 finished after 200 episode steps with total reward = -3142.000000.\n",
      "Episode 8/1000 finished after 200 episode steps with total reward = -3184.000000.\n",
      "Episode 9/1000 finished after 200 episode steps with total reward = -3163.000000.\n",
      "Episode 10/1000 finished after 200 episode steps with total reward = -3058.000000.\n",
      "Episode 11/1000 finished after 200 episode steps with total reward = -3037.000000.\n",
      "Episode 12/1000 finished after 200 episode steps with total reward = -2953.000000.\n",
      "Episode 13/1000 finished after 200 episode steps with total reward = -3184.000000.\n",
      "Episode 14/1000 finished after 200 episode steps with total reward = -3016.000000.\n",
      "Episode 15/1000 finished after 200 episode steps with total reward = -3268.000000.\n",
      "Episode 16/1000 finished after 200 episode steps with total reward = -3289.000000.\n",
      "Episode 17/1000 finished after 200 episode steps with total reward = -3226.000000.\n",
      "Episode 18/1000 finished after 200 episode steps with total reward = -3268.000000.\n",
      "Episode 19/1000 finished after 200 episode steps with total reward = -3205.000000.\n",
      "Episode 20/1000 finished after 200 episode steps with total reward = -3016.000000.\n",
      "Episode 21/1000 finished after 200 episode steps with total reward = -3163.000000.\n",
      "Episode 22/1000 finished after 200 episode steps with total reward = -3184.000000.\n",
      "Episode 23/1000 finished after 200 episode steps with total reward = -3226.000000.\n",
      "Episode 24/1000 finished after 200 episode steps with total reward = -3142.000000.\n",
      "Episode 25/1000 finished after 200 episode steps with total reward = -3100.000000.\n",
      "Episode 26/1000 finished after 200 episode steps with total reward = -2911.000000.\n",
      "Episode 27/1000 finished after 200 episode steps with total reward = -3142.000000.\n",
      "Episode 28/1000 finished after 200 episode steps with total reward = -3205.000000.\n",
      "Episode 29/1000 finished after 200 episode steps with total reward = -3016.000000.\n",
      "Episode 30/1000 finished after 200 episode steps with total reward = -3184.000000.\n",
      "Episode 31/1000 finished after 200 episode steps with total reward = -2974.000000.\n",
      "Episode 32/1000 finished after 200 episode steps with total reward = -3121.000000.\n",
      "Episode 33/1000 finished after 200 episode steps with total reward = -2974.000000.\n",
      "Episode 34/1000 finished after 200 episode steps with total reward = -2995.000000.\n",
      "Episode 35/1000 finished after 200 episode steps with total reward = -3016.000000.\n",
      "Episode 36/1000 finished after 200 episode steps with total reward = -3079.000000.\n",
      "Episode 37/1000 finished after 200 episode steps with total reward = -3226.000000.\n",
      "Episode 38/1000 finished after 200 episode steps with total reward = -3058.000000.\n",
      "Episode 39/1000 finished after 200 episode steps with total reward = -3037.000000.\n",
      "Episode 40/1000 finished after 200 episode steps with total reward = -3037.000000.\n",
      "Episode 41/1000 finished after 200 episode steps with total reward = -3037.000000.\n",
      "Episode 42/1000 finished after 200 episode steps with total reward = -2953.000000.\n",
      "Episode 43/1000 finished after 200 episode steps with total reward = -3100.000000.\n",
      "Episode 44/1000 finished after 200 episode steps with total reward = -3016.000000.\n",
      "Episode 45/1000 finished after 200 episode steps with total reward = -2869.000000.\n",
      "Episode 46/1000 finished after 200 episode steps with total reward = -3205.000000.\n",
      "Episode 47/1000 finished after 200 episode steps with total reward = -3100.000000.\n",
      "Episode 48/1000 finished after 200 episode steps with total reward = -2974.000000.\n",
      "Episode 49/1000 finished after 200 episode steps with total reward = -3289.000000.\n",
      "Episode 50/1000 finished after 200 episode steps with total reward = -3016.000000.\n",
      "Episode 51/1000 finished after 200 episode steps with total reward = -2953.000000.\n",
      "Episode 52/1000 finished after 200 episode steps with total reward = -3268.000000.\n",
      "Episode 53/1000 finished after 200 episode steps with total reward = -2932.000000.\n",
      "Episode 54/1000 finished after 200 episode steps with total reward = -2953.000000.\n",
      "Episode 55/1000 finished after 200 episode steps with total reward = -3079.000000.\n",
      "Episode 56/1000 finished after 200 episode steps with total reward = -3142.000000.\n",
      "Episode 57/1000 finished after 200 episode steps with total reward = -3121.000000.\n",
      "Episode 58/1000 finished after 200 episode steps with total reward = -2869.000000.\n",
      "Episode 59/1000 finished after 200 episode steps with total reward = -3079.000000.\n",
      "Episode 60/1000 finished after 200 episode steps with total reward = -3079.000000.\n",
      "Episode 61/1000 finished after 200 episode steps with total reward = -2869.000000.\n",
      "Episode 62/1000 finished after 200 episode steps with total reward = -2974.000000.\n",
      "Episode 63/1000 finished after 200 episode steps with total reward = -3142.000000.\n",
      "Episode 64/1000 finished after 200 episode steps with total reward = -3226.000000.\n",
      "Episode 65/1000 finished after 200 episode steps with total reward = -3121.000000.\n",
      "Episode 66/1000 finished after 200 episode steps with total reward = -2932.000000.\n",
      "Episode 67/1000 finished after 200 episode steps with total reward = -3100.000000.\n",
      "Episode 68/1000 finished after 200 episode steps with total reward = -3163.000000.\n",
      "Episode 69/1000 finished after 200 episode steps with total reward = -2848.000000.\n",
      "Episode 70/1000 finished after 200 episode steps with total reward = -3100.000000.\n",
      "Episode 71/1000 finished after 200 episode steps with total reward = -2995.000000.\n",
      "Episode 72/1000 finished after 200 episode steps with total reward = -2995.000000.\n",
      "Episode 73/1000 finished after 200 episode steps with total reward = -3016.000000.\n",
      "Episode 74/1000 finished after 200 episode steps with total reward = -3016.000000.\n",
      "Episode 75/1000 finished after 200 episode steps with total reward = -2848.000000.\n",
      "Episode 76/1000 finished after 200 episode steps with total reward = -2953.000000.\n",
      "Episode 77/1000 finished after 200 episode steps with total reward = -2848.000000.\n",
      "Episode 78/1000 finished after 200 episode steps with total reward = -2869.000000.\n",
      "Episode 79/1000 finished after 200 episode steps with total reward = -3079.000000.\n",
      "Episode 80/1000 finished after 200 episode steps with total reward = -2806.000000.\n",
      "Episode 81/1000 finished after 200 episode steps with total reward = -3079.000000.\n",
      "Episode 82/1000 finished after 200 episode steps with total reward = -2995.000000.\n",
      "Episode 83/1000 finished after 200 episode steps with total reward = -2806.000000.\n",
      "Episode 84/1000 finished after 200 episode steps with total reward = -2869.000000.\n",
      "Episode 85/1000 finished after 200 episode steps with total reward = -2764.000000.\n",
      "Episode 86/1000 finished after 200 episode steps with total reward = -3100.000000.\n",
      "Episode 87/1000 finished after 200 episode steps with total reward = -2848.000000.\n",
      "Episode 88/1000 finished after 200 episode steps with total reward = -3037.000000.\n",
      "Episode 89/1000 finished after 200 episode steps with total reward = -3079.000000.\n",
      "Episode 90/1000 finished after 200 episode steps with total reward = -2827.000000.\n",
      "Episode 91/1000 finished after 200 episode steps with total reward = -2932.000000.\n",
      "Episode 92/1000 finished after 200 episode steps with total reward = -2890.000000.\n",
      "Episode 93/1000 finished after 200 episode steps with total reward = -2722.000000.\n",
      "Episode 94/1000 finished after 200 episode steps with total reward = -2995.000000.\n",
      "Episode 95/1000 finished after 200 episode steps with total reward = -2911.000000.\n",
      "Episode 96/1000 finished after 200 episode steps with total reward = -2764.000000.\n",
      "Episode 97/1000 finished after 200 episode steps with total reward = -3058.000000.\n",
      "Episode 98/1000 finished after 200 episode steps with total reward = -2785.000000.\n",
      "Episode 99/1000 finished after 200 episode steps with total reward = -2785.000000.\n",
      "Episode 100/1000 finished after 200 episode steps with total reward = -2848.000000.\n",
      "Episode 101/1000 finished after 200 episode steps with total reward = -2869.000000.\n",
      "Episode 102/1000 finished after 200 episode steps with total reward = -2806.000000.\n",
      "Episode 103/1000 finished after 200 episode steps with total reward = -2743.000000.\n",
      "Episode 104/1000 finished after 200 episode steps with total reward = -2911.000000.\n",
      "Episode 105/1000 finished after 200 episode steps with total reward = -2806.000000.\n",
      "Episode 106/1000 finished after 200 episode steps with total reward = -3016.000000.\n",
      "Episode 107/1000 finished after 200 episode steps with total reward = -2701.000000.\n",
      "Episode 108/1000 finished after 200 episode steps with total reward = -2827.000000.\n",
      "Episode 109/1000 finished after 200 episode steps with total reward = -2785.000000.\n",
      "Episode 110/1000 finished after 200 episode steps with total reward = -2533.000000.\n",
      "Episode 111/1000 finished after 200 episode steps with total reward = -2995.000000.\n",
      "Episode 112/1000 finished after 200 episode steps with total reward = -2848.000000.\n",
      "Episode 113/1000 finished after 200 episode steps with total reward = -2953.000000.\n",
      "Episode 114/1000 finished after 200 episode steps with total reward = -2722.000000.\n",
      "Episode 115/1000 finished after 200 episode steps with total reward = -2806.000000.\n",
      "Episode 116/1000 finished after 200 episode steps with total reward = -2848.000000.\n",
      "Episode 117/1000 finished after 200 episode steps with total reward = -2974.000000.\n",
      "Episode 118/1000 finished after 200 episode steps with total reward = -2869.000000.\n",
      "Episode 119/1000 finished after 200 episode steps with total reward = -2638.000000.\n",
      "Episode 120/1000 finished after 200 episode steps with total reward = -2932.000000.\n",
      "Episode 121/1000 finished after 200 episode steps with total reward = -2743.000000.\n",
      "Episode 122/1000 finished after 200 episode steps with total reward = -2785.000000.\n",
      "Episode 123/1000 finished after 200 episode steps with total reward = -2827.000000.\n",
      "Episode 124/1000 finished after 200 episode steps with total reward = -2890.000000.\n",
      "Episode 125/1000 finished after 200 episode steps with total reward = -2806.000000.\n",
      "Episode 126/1000 finished after 200 episode steps with total reward = -2827.000000.\n",
      "Episode 127/1000 finished after 200 episode steps with total reward = -2785.000000.\n",
      "Episode 128/1000 finished after 200 episode steps with total reward = -2743.000000.\n",
      "Episode 129/1000 finished after 200 episode steps with total reward = -2953.000000.\n",
      "Episode 130/1000 finished after 200 episode steps with total reward = -2953.000000.\n",
      "Episode 131/1000 finished after 200 episode steps with total reward = -2680.000000.\n",
      "Episode 132/1000 finished after 200 episode steps with total reward = -2848.000000.\n",
      "Episode 133/1000 finished after 200 episode steps with total reward = -2890.000000.\n",
      "Episode 134/1000 finished after 200 episode steps with total reward = -2764.000000.\n",
      "Episode 135/1000 finished after 200 episode steps with total reward = -2827.000000.\n",
      "Episode 136/1000 finished after 200 episode steps with total reward = -2659.000000.\n",
      "Episode 137/1000 finished after 200 episode steps with total reward = -2701.000000.\n",
      "Episode 138/1000 finished after 200 episode steps with total reward = -2743.000000.\n",
      "Episode 139/1000 finished after 200 episode steps with total reward = -2722.000000.\n",
      "Episode 140/1000 finished after 200 episode steps with total reward = -2932.000000.\n",
      "Episode 141/1000 finished after 200 episode steps with total reward = -2806.000000.\n",
      "Episode 142/1000 finished after 200 episode steps with total reward = -2848.000000.\n",
      "Episode 143/1000 finished after 200 episode steps with total reward = -2680.000000.\n",
      "Episode 144/1000 finished after 200 episode steps with total reward = -2785.000000.\n",
      "Episode 145/1000 finished after 200 episode steps with total reward = -2827.000000.\n",
      "Episode 146/1000 finished after 200 episode steps with total reward = -2890.000000.\n",
      "Episode 147/1000 finished after 200 episode steps with total reward = -2701.000000.\n",
      "Episode 148/1000 finished after 200 episode steps with total reward = -2806.000000.\n",
      "Episode 149/1000 finished after 200 episode steps with total reward = -2701.000000.\n",
      "Episode 150/1000 finished after 200 episode steps with total reward = -2827.000000.\n",
      "Episode 151/1000 finished after 200 episode steps with total reward = -2722.000000.\n",
      "Episode 152/1000 finished after 200 episode steps with total reward = -2743.000000.\n",
      "Episode 153/1000 finished after 200 episode steps with total reward = -2785.000000.\n",
      "Episode 154/1000 finished after 200 episode steps with total reward = -2890.000000.\n",
      "Episode 155/1000 finished after 200 episode steps with total reward = -2638.000000.\n",
      "Episode 156/1000 finished after 200 episode steps with total reward = -2785.000000.\n",
      "Episode 157/1000 finished after 200 episode steps with total reward = -2722.000000.\n",
      "Episode 158/1000 finished after 200 episode steps with total reward = -2785.000000.\n",
      "Episode 159/1000 finished after 200 episode steps with total reward = -2806.000000.\n",
      "Episode 160/1000 finished after 200 episode steps with total reward = -2848.000000.\n",
      "Episode 161/1000 finished after 200 episode steps with total reward = -2722.000000.\n",
      "Episode 162/1000 finished after 200 episode steps with total reward = -2701.000000.\n",
      "Episode 163/1000 finished after 200 episode steps with total reward = -2659.000000.\n",
      "Episode 164/1000 finished after 200 episode steps with total reward = -2743.000000.\n",
      "Episode 165/1000 finished after 200 episode steps with total reward = -2806.000000.\n",
      "Episode 166/1000 finished after 200 episode steps with total reward = -2764.000000.\n",
      "Episode 167/1000 finished after 200 episode steps with total reward = -2596.000000.\n",
      "Episode 168/1000 finished after 200 episode steps with total reward = -2617.000000.\n",
      "Episode 169/1000 finished after 200 episode steps with total reward = -2869.000000.\n",
      "Episode 170/1000 finished after 200 episode steps with total reward = -2701.000000.\n",
      "Episode 171/1000 finished after 200 episode steps with total reward = -2764.000000.\n",
      "Episode 172/1000 finished after 200 episode steps with total reward = -2869.000000.\n",
      "Episode 173/1000 finished after 200 episode steps with total reward = -2701.000000.\n",
      "Episode 174/1000 finished after 200 episode steps with total reward = -2785.000000.\n",
      "Episode 175/1000 finished after 200 episode steps with total reward = -2470.000000.\n",
      "Episode 176/1000 finished after 200 episode steps with total reward = -2596.000000.\n",
      "Episode 177/1000 finished after 200 episode steps with total reward = -2827.000000.\n",
      "Episode 178/1000 finished after 200 episode steps with total reward = -2596.000000.\n",
      "Episode 179/1000 finished after 200 episode steps with total reward = -2827.000000.\n",
      "Episode 180/1000 finished after 200 episode steps with total reward = -2827.000000.\n",
      "Episode 181/1000 finished after 200 episode steps with total reward = -2743.000000.\n",
      "Episode 182/1000 finished after 200 episode steps with total reward = -2764.000000.\n",
      "Episode 183/1000 finished after 200 episode steps with total reward = -2764.000000.\n",
      "Episode 184/1000 finished after 200 episode steps with total reward = -2512.000000.\n",
      "Episode 185/1000 finished after 200 episode steps with total reward = -2869.000000.\n",
      "Episode 186/1000 finished after 200 episode steps with total reward = -2596.000000.\n",
      "Episode 187/1000 finished after 200 episode steps with total reward = -2827.000000.\n",
      "Episode 188/1000 finished after 197 episode steps with total reward = 7297.000000.\n",
      "Episode 189/1000 finished after 200 episode steps with total reward = -2596.000000.\n",
      "Episode 190/1000 finished after 200 episode steps with total reward = -2701.000000.\n",
      "Episode 191/1000 finished after 200 episode steps with total reward = -2617.000000.\n",
      "Episode 192/1000 finished after 200 episode steps with total reward = -2743.000000.\n",
      "Episode 193/1000 finished after 200 episode steps with total reward = -2596.000000.\n",
      "Episode 194/1000 finished after 200 episode steps with total reward = -2638.000000.\n",
      "Episode 195/1000 finished after 200 episode steps with total reward = -2596.000000.\n",
      "Episode 196/1000 finished after 144 episode steps with total reward = 7982.000000.\n",
      "Episode 197/1000 finished after 139 episode steps with total reward = 8091.000000.\n",
      "Episode 198/1000 finished after 169 episode steps with total reward = 7689.000000.\n",
      "Episode 199/1000 finished after 171 episode steps with total reward = 7616.000000.\n",
      "Episode 200/1000 finished after 141 episode steps with total reward = 8018.000000.\n",
      "Episode 201/1000 finished after 200 episode steps with total reward = -2848.000000.\n",
      "Episode 202/1000 finished after 143 episode steps with total reward = 7924.000000.\n",
      "Episode 203/1000 finished after 200 episode steps with total reward = -2701.000000.\n",
      "Episode 204/1000 finished after 141 episode steps with total reward = 8018.000000.\n",
      "Episode 205/1000 finished after 138 episode steps with total reward = 8096.000000.\n",
      "Episode 206/1000 finished after 140 episode steps with total reward = 8044.000000.\n",
      "Episode 207/1000 finished after 137 episode steps with total reward = 8122.000000.\n",
      "Episode 208/1000 finished after 137 episode steps with total reward = 8122.000000.\n",
      "Episode 209/1000 finished after 136 episode steps with total reward = 8106.000000.\n",
      "Episode 210/1000 finished after 135 episode steps with total reward = 8153.000000.\n",
      "Episode 211/1000 finished after 139 episode steps with total reward = 8049.000000.\n",
      "Episode 212/1000 finished after 140 episode steps with total reward = 8044.000000.\n",
      "Episode 213/1000 finished after 144 episode steps with total reward = 7961.000000.\n",
      "Episode 214/1000 finished after 141 episode steps with total reward = 8018.000000.\n",
      "Episode 215/1000 finished after 144 episode steps with total reward = 7982.000000.\n",
      "Episode 216/1000 finished after 142 episode steps with total reward = 7971.000000.\n",
      "Episode 217/1000 finished after 141 episode steps with total reward = 8018.000000.\n",
      "Episode 218/1000 finished after 140 episode steps with total reward = 8044.000000.\n",
      "Episode 219/1000 finished after 141 episode steps with total reward = 8081.000000.\n",
      "Episode 220/1000 finished after 142 episode steps with total reward = 8013.000000.\n",
      "Episode 221/1000 finished after 138 episode steps with total reward = 8096.000000.\n",
      "Episode 222/1000 finished after 146 episode steps with total reward = 7930.000000.\n",
      "Episode 223/1000 finished after 146 episode steps with total reward = 7930.000000.\n",
      "Episode 224/1000 finished after 148 episode steps with total reward = 7920.000000.\n",
      "Episode 225/1000 finished after 139 episode steps with total reward = 8070.000000.\n",
      "Episode 226/1000 finished after 146 episode steps with total reward = 7972.000000.\n",
      "Episode 227/1000 finished after 141 episode steps with total reward = 8018.000000.\n",
      "Episode 228/1000 finished after 140 episode steps with total reward = 8065.000000.\n",
      "Episode 229/1000 finished after 143 episode steps with total reward = 7966.000000.\n",
      "Episode 230/1000 finished after 145 episode steps with total reward = 7977.000000.\n",
      "Episode 231/1000 finished after 142 episode steps with total reward = 8013.000000.\n",
      "Episode 232/1000 finished after 137 episode steps with total reward = 8122.000000.\n",
      "Episode 233/1000 finished after 143 episode steps with total reward = 7987.000000.\n",
      "Episode 234/1000 finished after 145 episode steps with total reward = 7977.000000.\n",
      "Episode 235/1000 finished after 138 episode steps with total reward = 8096.000000.\n",
      "Episode 236/1000 finished after 137 episode steps with total reward = 8122.000000.\n",
      "Episode 237/1000 finished after 138 episode steps with total reward = 8075.000000.\n",
      "Episode 238/1000 finished after 125 episode steps with total reward = 8245.000000.\n",
      "Episode 239/1000 finished after 116 episode steps with total reward = 8185.000000.\n",
      "Episode 240/1000 finished after 180 episode steps with total reward = 7550.000000.\n",
      "Episode 241/1000 finished after 138 episode steps with total reward = 8117.000000.\n",
      "Episode 242/1000 finished after 137 episode steps with total reward = 8122.000000.\n",
      "Episode 243/1000 finished after 165 episode steps with total reward = 7835.000000.\n",
      "Episode 244/1000 finished after 178 episode steps with total reward = 7539.000000.\n",
      "Episode 245/1000 finished after 167 episode steps with total reward = 7720.000000.\n",
      "Episode 246/1000 finished after 142 episode steps with total reward = 7992.000000.\n",
      "Episode 247/1000 finished after 168 episode steps with total reward = 7631.000000.\n",
      "Episode 248/1000 finished after 170 episode steps with total reward = 7642.000000.\n",
      "Episode 249/1000 finished after 200 episode steps with total reward = -2701.000000.\n",
      "Episode 250/1000 finished after 176 episode steps with total reward = 7549.000000.\n",
      "Episode 251/1000 finished after 168 episode steps with total reward = 7736.000000.\n",
      "Episode 252/1000 finished after 200 episode steps with total reward = -2764.000000.\n",
      "Episode 253/1000 finished after 200 episode steps with total reward = -2806.000000.\n",
      "Episode 254/1000 finished after 169 episode steps with total reward = 7647.000000.\n",
      "Episode 255/1000 finished after 163 episode steps with total reward = 7761.000000.\n",
      "Episode 256/1000 finished after 190 episode steps with total reward = 7374.000000.\n",
      "Episode 257/1000 finished after 200 episode steps with total reward = -2848.000000.\n",
      "Episode 258/1000 finished after 172 episode steps with total reward = 7611.000000.\n",
      "Episode 259/1000 finished after 200 episode steps with total reward = -2806.000000.\n",
      "Episode 260/1000 finished after 199 episode steps with total reward = 7287.000000.\n",
      "Episode 261/1000 finished after 200 episode steps with total reward = -2785.000000.\n",
      "Episode 262/1000 finished after 200 episode steps with total reward = -2785.000000.\n",
      "Episode 263/1000 finished after 200 episode steps with total reward = -2827.000000.\n",
      "Episode 264/1000 finished after 200 episode steps with total reward = -2806.000000.\n",
      "Episode 265/1000 finished after 200 episode steps with total reward = -2869.000000.\n",
      "Episode 266/1000 finished after 200 episode steps with total reward = -2890.000000.\n",
      "Episode 267/1000 finished after 200 episode steps with total reward = -2638.000000.\n",
      "Episode 268/1000 finished after 200 episode steps with total reward = -2659.000000.\n",
      "Episode 269/1000 finished after 200 episode steps with total reward = -2554.000000.\n",
      "Episode 270/1000 finished after 200 episode steps with total reward = -2638.000000.\n",
      "Episode 271/1000 finished after 200 episode steps with total reward = -2617.000000.\n",
      "Episode 272/1000 finished after 200 episode steps with total reward = -2680.000000.\n",
      "Episode 273/1000 finished after 200 episode steps with total reward = -2659.000000.\n",
      "Episode 274/1000 finished after 200 episode steps with total reward = -2743.000000.\n",
      "Episode 275/1000 finished after 200 episode steps with total reward = -2617.000000.\n",
      "Episode 276/1000 finished after 200 episode steps with total reward = -2722.000000.\n",
      "Episode 277/1000 finished after 200 episode steps with total reward = -2722.000000.\n",
      "Episode 278/1000 finished after 200 episode steps with total reward = -2638.000000.\n",
      "Episode 279/1000 finished after 200 episode steps with total reward = -2806.000000.\n",
      "Episode 280/1000 finished after 200 episode steps with total reward = -2848.000000.\n",
      "Episode 281/1000 finished after 200 episode steps with total reward = -2890.000000.\n",
      "Episode 282/1000 finished after 200 episode steps with total reward = -2743.000000.\n",
      "Episode 283/1000 finished after 200 episode steps with total reward = -2659.000000.\n",
      "Episode 284/1000 finished after 200 episode steps with total reward = -2638.000000.\n",
      "Episode 285/1000 finished after 200 episode steps with total reward = -2596.000000.\n",
      "Episode 286/1000 finished after 200 episode steps with total reward = -2722.000000.\n",
      "Episode 287/1000 finished after 200 episode steps with total reward = -2659.000000.\n",
      "Episode 288/1000 finished after 200 episode steps with total reward = -2827.000000.\n",
      "Episode 289/1000 finished after 200 episode steps with total reward = -2617.000000.\n",
      "Episode 290/1000 finished after 200 episode steps with total reward = -2680.000000.\n",
      "Episode 291/1000 finished after 200 episode steps with total reward = -2680.000000.\n",
      "Episode 292/1000 finished after 200 episode steps with total reward = -2617.000000.\n",
      "Episode 293/1000 finished after 200 episode steps with total reward = -2638.000000.\n",
      "Episode 294/1000 finished after 200 episode steps with total reward = -2890.000000.\n",
      "Episode 295/1000 finished after 200 episode steps with total reward = -2806.000000.\n",
      "Episode 296/1000 finished after 200 episode steps with total reward = -2701.000000.\n",
      "Episode 297/1000 finished after 200 episode steps with total reward = -2680.000000.\n",
      "Episode 298/1000 finished after 200 episode steps with total reward = -2680.000000.\n",
      "Episode 299/1000 finished after 200 episode steps with total reward = -2848.000000.\n",
      "Episode 300/1000 finished after 200 episode steps with total reward = -2848.000000.\n",
      "Episode 301/1000 finished after 200 episode steps with total reward = -2722.000000.\n",
      "Episode 302/1000 finished after 200 episode steps with total reward = -2785.000000.\n",
      "Episode 303/1000 finished after 200 episode steps with total reward = -2764.000000.\n",
      "Episode 304/1000 finished after 200 episode steps with total reward = -2722.000000.\n",
      "Episode 305/1000 finished after 178 episode steps with total reward = 7434.000000.\n",
      "Episode 306/1000 finished after 200 episode steps with total reward = -2827.000000.\n",
      "Episode 307/1000 finished after 176 episode steps with total reward = 7465.000000.\n",
      "Episode 308/1000 finished after 200 episode steps with total reward = -2722.000000.\n",
      "Episode 309/1000 finished after 183 episode steps with total reward = 7346.000000.\n",
      "Episode 310/1000 finished after 182 episode steps with total reward = 7351.000000.\n",
      "Episode 311/1000 finished after 182 episode steps with total reward = 7309.000000.\n",
      "Episode 312/1000 finished after 189 episode steps with total reward = 7190.000000.\n",
      "Episode 313/1000 finished after 177 episode steps with total reward = 7439.000000.\n",
      "Episode 314/1000 finished after 184 episode steps with total reward = 7299.000000.\n",
      "Episode 315/1000 finished after 183 episode steps with total reward = 7304.000000.\n",
      "Episode 316/1000 finished after 184 episode steps with total reward = 7299.000000.\n",
      "Episode 317/1000 finished after 179 episode steps with total reward = 7387.000000.\n",
      "Episode 318/1000 finished after 190 episode steps with total reward = 7227.000000.\n",
      "Episode 319/1000 finished after 186 episode steps with total reward = 7226.000000.\n",
      "Episode 320/1000 finished after 184 episode steps with total reward = 7278.000000.\n",
      "Episode 321/1000 finished after 187 episode steps with total reward = 7221.000000.\n",
      "Episode 322/1000 finished after 200 episode steps with total reward = -3058.000000.\n",
      "Episode 323/1000 finished after 200 episode steps with total reward = -3121.000000.\n",
      "Episode 324/1000 finished after 200 episode steps with total reward = -3016.000000.\n",
      "Episode 325/1000 finished after 200 episode steps with total reward = -3079.000000.\n",
      "Episode 326/1000 finished after 200 episode steps with total reward = -3058.000000.\n",
      "Episode 327/1000 finished after 200 episode steps with total reward = -3100.000000.\n",
      "Episode 328/1000 finished after 200 episode steps with total reward = -3121.000000.\n",
      "Episode 329/1000 finished after 200 episode steps with total reward = -3100.000000.\n",
      "Episode 330/1000 finished after 200 episode steps with total reward = -3058.000000.\n",
      "Episode 331/1000 finished after 200 episode steps with total reward = -3058.000000.\n",
      "Episode 332/1000 finished after 200 episode steps with total reward = -3142.000000.\n",
      "Episode 333/1000 finished after 200 episode steps with total reward = -3121.000000.\n",
      "Episode 334/1000 finished after 200 episode steps with total reward = -3079.000000.\n",
      "Episode 335/1000 finished after 200 episode steps with total reward = -3079.000000.\n",
      "Episode 336/1000 finished after 200 episode steps with total reward = -3142.000000.\n",
      "Episode 337/1000 finished after 200 episode steps with total reward = -3184.000000.\n",
      "Episode 338/1000 finished after 200 episode steps with total reward = -3100.000000.\n",
      "Episode 339/1000 finished after 200 episode steps with total reward = -3058.000000.\n",
      "Episode 340/1000 finished after 200 episode steps with total reward = -3100.000000.\n",
      "Episode 341/1000 finished after 200 episode steps with total reward = -3184.000000.\n",
      "Episode 342/1000 finished after 200 episode steps with total reward = -2932.000000.\n",
      "Episode 343/1000 finished after 200 episode steps with total reward = -2995.000000.\n",
      "Episode 344/1000 finished after 200 episode steps with total reward = -3205.000000.\n",
      "Episode 345/1000 finished after 200 episode steps with total reward = -3184.000000.\n",
      "Episode 346/1000 finished after 200 episode steps with total reward = -3100.000000.\n",
      "Episode 347/1000 finished after 200 episode steps with total reward = -2953.000000.\n",
      "Episode 348/1000 finished after 200 episode steps with total reward = -2995.000000.\n",
      "Episode 349/1000 finished after 200 episode steps with total reward = -3121.000000.\n",
      "Episode 350/1000 finished after 200 episode steps with total reward = -3142.000000.\n",
      "Episode 351/1000 finished after 200 episode steps with total reward = -3121.000000.\n",
      "Episode 352/1000 finished after 200 episode steps with total reward = -3184.000000.\n",
      "Episode 353/1000 finished after 200 episode steps with total reward = -3142.000000.\n",
      "Episode 354/1000 finished after 200 episode steps with total reward = -3100.000000.\n",
      "Episode 355/1000 finished after 200 episode steps with total reward = -3037.000000.\n",
      "Episode 356/1000 finished after 200 episode steps with total reward = -3037.000000.\n",
      "Episode 357/1000 finished after 200 episode steps with total reward = -3037.000000.\n",
      "Episode 358/1000 finished after 200 episode steps with total reward = -3121.000000.\n",
      "Episode 359/1000 finished after 200 episode steps with total reward = -3079.000000.\n",
      "Episode 360/1000 finished after 200 episode steps with total reward = -3100.000000.\n",
      "Episode 361/1000 finished after 200 episode steps with total reward = -3037.000000.\n",
      "Episode 362/1000 finished after 200 episode steps with total reward = -3079.000000.\n",
      "Episode 363/1000 finished after 200 episode steps with total reward = -3121.000000.\n",
      "Episode 364/1000 finished after 200 episode steps with total reward = -3205.000000.\n",
      "Episode 365/1000 finished after 200 episode steps with total reward = -3037.000000.\n",
      "Episode 366/1000 finished after 200 episode steps with total reward = -2953.000000.\n",
      "Episode 367/1000 finished after 200 episode steps with total reward = -3163.000000.\n",
      "Episode 368/1000 finished after 200 episode steps with total reward = -2869.000000.\n",
      "Episode 369/1000 finished after 200 episode steps with total reward = -3142.000000.\n",
      "Episode 370/1000 finished after 200 episode steps with total reward = -3058.000000.\n",
      "Episode 371/1000 finished after 200 episode steps with total reward = -2995.000000.\n",
      "Episode 372/1000 finished after 200 episode steps with total reward = -3079.000000.\n",
      "Episode 373/1000 finished after 200 episode steps with total reward = -2995.000000.\n",
      "Episode 374/1000 finished after 200 episode steps with total reward = -3184.000000.\n",
      "Episode 375/1000 finished after 200 episode steps with total reward = -2995.000000.\n",
      "Episode 376/1000 finished after 200 episode steps with total reward = -2848.000000.\n",
      "Episode 377/1000 finished after 200 episode steps with total reward = -3058.000000.\n",
      "Episode 378/1000 finished after 182 episode steps with total reward = 7162.000000.\n",
      "Episode 379/1000 finished after 200 episode steps with total reward = -3079.000000.\n",
      "Episode 380/1000 finished after 200 episode steps with total reward = -2932.000000.\n",
      "Episode 381/1000 finished after 165 episode steps with total reward = 7457.000000.\n",
      "Episode 382/1000 finished after 173 episode steps with total reward = 7291.000000.\n",
      "Episode 383/1000 finished after 200 episode steps with total reward = -3226.000000.\n",
      "Episode 384/1000 finished after 196 episode steps with total reward = 7092.000000.\n",
      "Episode 385/1000 finished after 190 episode steps with total reward = 7185.000000.\n",
      "Episode 386/1000 finished after 165 episode steps with total reward = 7499.000000.\n",
      "Episode 387/1000 finished after 182 episode steps with total reward = 7309.000000.\n",
      "Episode 388/1000 finished after 200 episode steps with total reward = -2974.000000.\n",
      "Episode 389/1000 finished after 166 episode steps with total reward = 7452.000000.\n",
      "Episode 390/1000 finished after 170 episode steps with total reward = 7327.000000.\n",
      "Episode 391/1000 finished after 166 episode steps with total reward = 7410.000000.\n",
      "Episode 392/1000 finished after 164 episode steps with total reward = 7483.000000.\n",
      "Episode 393/1000 finished after 200 episode steps with total reward = -2848.000000.\n",
      "Episode 394/1000 finished after 200 episode steps with total reward = -2953.000000.\n",
      "Episode 395/1000 finished after 200 episode steps with total reward = -2911.000000.\n",
      "Episode 396/1000 finished after 200 episode steps with total reward = -2953.000000.\n",
      "Episode 397/1000 finished after 200 episode steps with total reward = -2974.000000.\n",
      "Episode 398/1000 finished after 200 episode steps with total reward = -2890.000000.\n",
      "Episode 399/1000 finished after 200 episode steps with total reward = -2890.000000.\n",
      "Episode 400/1000 finished after 200 episode steps with total reward = -2869.000000.\n",
      "Episode 401/1000 finished after 200 episode steps with total reward = -3016.000000.\n",
      "Episode 402/1000 finished after 200 episode steps with total reward = -3037.000000.\n",
      "Episode 403/1000 finished after 200 episode steps with total reward = -3037.000000.\n",
      "Episode 404/1000 finished after 200 episode steps with total reward = -2911.000000.\n",
      "Episode 405/1000 finished after 200 episode steps with total reward = -2806.000000.\n",
      "Episode 406/1000 finished after 200 episode steps with total reward = -2974.000000.\n",
      "Episode 407/1000 finished after 200 episode steps with total reward = -2974.000000.\n",
      "Episode 408/1000 finished after 165 episode steps with total reward = 7457.000000.\n",
      "Episode 409/1000 finished after 200 episode steps with total reward = -2890.000000.\n",
      "Episode 410/1000 finished after 200 episode steps with total reward = -3079.000000.\n",
      "Episode 411/1000 finished after 200 episode steps with total reward = -2911.000000.\n",
      "Episode 412/1000 finished after 200 episode steps with total reward = -3058.000000.\n",
      "Episode 413/1000 finished after 200 episode steps with total reward = -2848.000000.\n",
      "Episode 414/1000 finished after 200 episode steps with total reward = -2890.000000.\n",
      "Episode 415/1000 finished after 200 episode steps with total reward = -3037.000000.\n",
      "Episode 416/1000 finished after 200 episode steps with total reward = -2806.000000.\n",
      "Episode 417/1000 finished after 200 episode steps with total reward = -2995.000000.\n",
      "Episode 418/1000 finished after 200 episode steps with total reward = -3016.000000.\n",
      "Episode 419/1000 finished after 200 episode steps with total reward = -2995.000000.\n",
      "Episode 420/1000 finished after 200 episode steps with total reward = -2953.000000.\n",
      "Episode 421/1000 finished after 200 episode steps with total reward = -2953.000000.\n",
      "Episode 422/1000 finished after 200 episode steps with total reward = -2974.000000.\n",
      "Episode 423/1000 finished after 200 episode steps with total reward = -2953.000000.\n",
      "Episode 424/1000 finished after 200 episode steps with total reward = -3205.000000.\n",
      "Episode 425/1000 finished after 200 episode steps with total reward = -2911.000000.\n",
      "Episode 426/1000 finished after 200 episode steps with total reward = -3079.000000.\n",
      "Episode 427/1000 finished after 200 episode steps with total reward = -3016.000000.\n",
      "Episode 428/1000 finished after 200 episode steps with total reward = -3079.000000.\n",
      "Episode 429/1000 finished after 200 episode steps with total reward = -3247.000000.\n",
      "Episode 430/1000 finished after 200 episode steps with total reward = -3079.000000.\n",
      "Episode 431/1000 finished after 200 episode steps with total reward = -2932.000000.\n",
      "Episode 432/1000 finished after 200 episode steps with total reward = -3205.000000.\n",
      "Episode 433/1000 finished after 200 episode steps with total reward = -3163.000000.\n",
      "Episode 434/1000 finished after 200 episode steps with total reward = -3268.000000.\n",
      "Episode 435/1000 finished after 200 episode steps with total reward = -3184.000000.\n",
      "Episode 436/1000 finished after 200 episode steps with total reward = -3100.000000.\n",
      "Episode 437/1000 finished after 200 episode steps with total reward = -3142.000000.\n",
      "Episode 438/1000 finished after 200 episode steps with total reward = -3121.000000.\n",
      "Episode 439/1000 finished after 200 episode steps with total reward = -2995.000000.\n",
      "Episode 440/1000 finished after 200 episode steps with total reward = -3121.000000.\n",
      "Episode 441/1000 finished after 200 episode steps with total reward = -3163.000000.\n",
      "Episode 442/1000 finished after 200 episode steps with total reward = -3163.000000.\n",
      "Episode 443/1000 finished after 200 episode steps with total reward = -3289.000000.\n",
      "Episode 444/1000 finished after 200 episode steps with total reward = -3058.000000.\n",
      "Episode 445/1000 finished after 200 episode steps with total reward = -3247.000000.\n",
      "Episode 446/1000 finished after 200 episode steps with total reward = -3352.000000.\n",
      "Episode 447/1000 finished after 200 episode steps with total reward = -2827.000000.\n",
      "Episode 448/1000 finished after 200 episode steps with total reward = -3121.000000.\n",
      "Episode 449/1000 finished after 200 episode steps with total reward = -3205.000000.\n",
      "Episode 450/1000 finished after 200 episode steps with total reward = -3016.000000.\n",
      "Episode 451/1000 finished after 200 episode steps with total reward = -3100.000000.\n",
      "Episode 452/1000 finished after 200 episode steps with total reward = -3142.000000.\n",
      "Episode 453/1000 finished after 200 episode steps with total reward = -3331.000000.\n",
      "Episode 454/1000 finished after 200 episode steps with total reward = -3058.000000.\n",
      "Episode 455/1000 finished after 200 episode steps with total reward = -3331.000000.\n",
      "Episode 456/1000 finished after 200 episode steps with total reward = -3331.000000.\n",
      "Episode 457/1000 finished after 200 episode steps with total reward = -3331.000000.\n",
      "Episode 458/1000 finished after 200 episode steps with total reward = -3268.000000.\n",
      "Episode 459/1000 finished after 200 episode steps with total reward = -3247.000000.\n",
      "Episode 460/1000 finished after 200 episode steps with total reward = -3205.000000.\n",
      "Episode 461/1000 finished after 200 episode steps with total reward = -3415.000000.\n",
      "Episode 462/1000 finished after 200 episode steps with total reward = -3331.000000.\n",
      "Episode 463/1000 finished after 200 episode steps with total reward = -3310.000000.\n",
      "Episode 464/1000 finished after 200 episode steps with total reward = -3247.000000.\n",
      "Episode 465/1000 finished after 200 episode steps with total reward = -3247.000000.\n",
      "Episode 466/1000 finished after 200 episode steps with total reward = -3289.000000.\n",
      "Episode 467/1000 finished after 200 episode steps with total reward = -3268.000000.\n",
      "Episode 468/1000 finished after 200 episode steps with total reward = -3037.000000.\n",
      "Episode 469/1000 finished after 200 episode steps with total reward = -3163.000000.\n",
      "Episode 470/1000 finished after 200 episode steps with total reward = -3184.000000.\n",
      "Episode 471/1000 finished after 200 episode steps with total reward = -3184.000000.\n",
      "Episode 472/1000 finished after 200 episode steps with total reward = -3184.000000.\n",
      "Episode 473/1000 finished after 200 episode steps with total reward = -3016.000000.\n",
      "Episode 474/1000 finished after 200 episode steps with total reward = -3121.000000.\n",
      "Episode 475/1000 finished after 200 episode steps with total reward = -3457.000000.\n",
      "Episode 476/1000 finished after 200 episode steps with total reward = -3037.000000.\n",
      "Episode 477/1000 finished after 200 episode steps with total reward = -3331.000000.\n",
      "Episode 478/1000 finished after 200 episode steps with total reward = -3079.000000.\n",
      "Episode 479/1000 finished after 200 episode steps with total reward = -3058.000000.\n",
      "Episode 480/1000 finished after 200 episode steps with total reward = -2995.000000.\n",
      "Episode 481/1000 finished after 200 episode steps with total reward = -3268.000000.\n",
      "Episode 482/1000 finished after 200 episode steps with total reward = -2890.000000.\n",
      "Episode 483/1000 finished after 200 episode steps with total reward = -3142.000000.\n",
      "Episode 484/1000 finished after 200 episode steps with total reward = -3163.000000.\n",
      "Episode 485/1000 finished after 200 episode steps with total reward = -3247.000000.\n",
      "Episode 486/1000 finished after 200 episode steps with total reward = -3058.000000.\n",
      "Episode 487/1000 finished after 200 episode steps with total reward = -3037.000000.\n",
      "Episode 488/1000 finished after 200 episode steps with total reward = -3037.000000.\n",
      "Episode 489/1000 finished after 200 episode steps with total reward = -2743.000000.\n",
      "Episode 490/1000 finished after 200 episode steps with total reward = -3205.000000.\n",
      "Episode 491/1000 finished after 200 episode steps with total reward = -3163.000000.\n",
      "Episode 492/1000 finished after 200 episode steps with total reward = -2806.000000.\n",
      "Episode 493/1000 finished after 200 episode steps with total reward = -3163.000000.\n",
      "Episode 494/1000 finished after 200 episode steps with total reward = -2974.000000.\n",
      "Episode 495/1000 finished after 200 episode steps with total reward = -3016.000000.\n",
      "Episode 496/1000 finished after 200 episode steps with total reward = -3016.000000.\n",
      "Episode 497/1000 finished after 200 episode steps with total reward = -2890.000000.\n",
      "Episode 498/1000 finished after 200 episode steps with total reward = -3100.000000.\n",
      "Episode 499/1000 finished after 200 episode steps with total reward = -2869.000000.\n",
      "Episode 500/1000 finished after 200 episode steps with total reward = -2827.000000.\n",
      "Episode 501/1000 finished after 200 episode steps with total reward = -3079.000000.\n",
      "Episode 502/1000 finished after 200 episode steps with total reward = -3079.000000.\n",
      "Episode 503/1000 finished after 200 episode steps with total reward = -2869.000000.\n",
      "Episode 504/1000 finished after 200 episode steps with total reward = -2974.000000.\n",
      "Episode 505/1000 finished after 200 episode steps with total reward = -3121.000000.\n",
      "Episode 506/1000 finished after 200 episode steps with total reward = -3058.000000.\n",
      "Episode 507/1000 finished after 186 episode steps with total reward = 7184.000000.\n",
      "Episode 508/1000 finished after 200 episode steps with total reward = -3037.000000.\n",
      "Episode 509/1000 finished after 158 episode steps with total reward = 7744.000000.\n",
      "Episode 510/1000 finished after 200 episode steps with total reward = -2995.000000.\n",
      "Episode 511/1000 finished after 200 episode steps with total reward = -2743.000000.\n",
      "Episode 512/1000 finished after 200 episode steps with total reward = -2890.000000.\n",
      "Episode 513/1000 finished after 200 episode steps with total reward = -2932.000000.\n",
      "Episode 514/1000 finished after 200 episode steps with total reward = -2974.000000.\n",
      "Episode 515/1000 finished after 200 episode steps with total reward = -2932.000000.\n",
      "Episode 516/1000 finished after 200 episode steps with total reward = -3247.000000.\n",
      "Episode 517/1000 finished after 200 episode steps with total reward = -2617.000000.\n",
      "Episode 518/1000 finished after 200 episode steps with total reward = -2932.000000.\n",
      "Episode 519/1000 finished after 200 episode steps with total reward = -2974.000000.\n",
      "Episode 520/1000 finished after 200 episode steps with total reward = -3100.000000.\n",
      "Episode 521/1000 finished after 200 episode steps with total reward = -3184.000000.\n",
      "Episode 522/1000 finished after 154 episode steps with total reward = 7953.000000.\n",
      "Episode 523/1000 finished after 200 episode steps with total reward = -3058.000000.\n",
      "Episode 524/1000 finished after 151 episode steps with total reward = 8073.000000.\n",
      "Episode 525/1000 finished after 200 episode steps with total reward = -3016.000000.\n",
      "Episode 526/1000 finished after 200 episode steps with total reward = -2953.000000.\n",
      "Episode 527/1000 finished after 200 episode steps with total reward = -2806.000000.\n",
      "Episode 528/1000 finished after 153 episode steps with total reward = 8063.000000.\n",
      "Episode 529/1000 finished after 146 episode steps with total reward = 8035.000000.\n",
      "Episode 530/1000 finished after 200 episode steps with total reward = -2869.000000.\n",
      "Episode 531/1000 finished after 200 episode steps with total reward = -2911.000000.\n",
      "Episode 532/1000 finished after 200 episode steps with total reward = -3016.000000.\n",
      "Episode 533/1000 finished after 153 episode steps with total reward = 8021.000000.\n",
      "Episode 534/1000 finished after 194 episode steps with total reward = 7354.000000.\n",
      "Episode 535/1000 finished after 200 episode steps with total reward = -3142.000000.\n",
      "Episode 536/1000 finished after 200 episode steps with total reward = -2764.000000.\n",
      "Episode 537/1000 finished after 200 episode steps with total reward = -2995.000000.\n",
      "Episode 538/1000 finished after 200 episode steps with total reward = -2848.000000.\n",
      "Episode 539/1000 finished after 200 episode steps with total reward = -2953.000000.\n",
      "Episode 540/1000 finished after 200 episode steps with total reward = -3184.000000.\n",
      "Episode 541/1000 finished after 189 episode steps with total reward = 7358.000000.\n",
      "Episode 542/1000 finished after 200 episode steps with total reward = -2638.000000.\n",
      "Episode 543/1000 finished after 200 episode steps with total reward = -3037.000000.\n",
      "Episode 544/1000 finished after 155 episode steps with total reward = 8011.000000.\n",
      "Episode 545/1000 finished after 200 episode steps with total reward = -3037.000000.\n",
      "Episode 546/1000 finished after 156 episode steps with total reward = 7964.000000.\n",
      "Episode 547/1000 finished after 200 episode steps with total reward = -2848.000000.\n",
      "Episode 548/1000 finished after 180 episode steps with total reward = 7508.000000.\n",
      "Episode 549/1000 finished after 182 episode steps with total reward = 7393.000000.\n",
      "Episode 550/1000 finished after 200 episode steps with total reward = -3100.000000.\n",
      "Episode 551/1000 finished after 159 episode steps with total reward = 7907.000000.\n",
      "Episode 552/1000 finished after 200 episode steps with total reward = -2638.000000.\n",
      "Episode 553/1000 finished after 158 episode steps with total reward = 7996.000000.\n",
      "Episode 554/1000 finished after 184 episode steps with total reward = 7509.000000.\n",
      "Episode 555/1000 finished after 178 episode steps with total reward = 7497.000000.\n",
      "Episode 556/1000 finished after 200 episode steps with total reward = -2785.000000.\n",
      "Episode 557/1000 finished after 155 episode steps with total reward = 7990.000000.\n",
      "Episode 558/1000 finished after 187 episode steps with total reward = 7410.000000.\n",
      "Episode 559/1000 finished after 200 episode steps with total reward = -2890.000000.\n",
      "Episode 560/1000 finished after 149 episode steps with total reward = 8020.000000.\n",
      "Episode 561/1000 finished after 200 episode steps with total reward = -2722.000000.\n",
      "Episode 562/1000 finished after 171 episode steps with total reward = 7553.000000.\n",
      "Episode 563/1000 finished after 200 episode steps with total reward = -2701.000000.\n",
      "Episode 564/1000 finished after 200 episode steps with total reward = -2932.000000.\n",
      "Episode 565/1000 finished after 184 episode steps with total reward = 7446.000000.\n",
      "Episode 566/1000 finished after 157 episode steps with total reward = 7917.000000.\n",
      "Episode 567/1000 finished after 200 episode steps with total reward = -2785.000000.\n",
      "Episode 568/1000 finished after 182 episode steps with total reward = 7330.000000.\n",
      "Episode 569/1000 finished after 184 episode steps with total reward = 7467.000000.\n",
      "Episode 570/1000 finished after 180 episode steps with total reward = 7445.000000.\n",
      "Episode 571/1000 finished after 156 episode steps with total reward = 7922.000000.\n",
      "Episode 572/1000 finished after 193 episode steps with total reward = 7275.000000.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3788289e69ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;31m# Updates the network weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3788289e69ab>\u001b[0m in \u001b[0;36mexperience_replay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         )\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1146\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1381\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    346\u001b[0m     dataset = tf.data.Dataset.zip((\n\u001b[1;32m    347\u001b[0m         \u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     ))\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensors\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \"\"\"\n\u001b[0;32m--> 606\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   3829\u001b[0m     variant_tensor = gen_dataset_ops.tensor_dataset(\n\u001b[1;32m   3830\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3831\u001b[0;31m         output_shapes=structure.get_flat_tensor_shapes(self._structure))\n\u001b[0m\u001b[1;32m   3832\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mtensor_dataset\u001b[0;34m(components, output_shapes, name)\u001b[0m\n\u001b[1;32m   7039\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   7040\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TensorDataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7041\u001b[0;31m         output_shapes)\n\u001b[0m\u001b[1;32m   7042\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7043\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MountainCar-v0 -- Deep Q-learning\n",
    "\"\"\"\n",
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, state_size, action_size, batch_size=32, memory_size=50000):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.training = 10000  # training after 10000 env steps\n",
    "        self.gamma = 0.95  # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "    def experience_replay(self):\n",
    "        # Updates the online network weights after enough data is collected\n",
    "        if self.training >= len(self.memory):\n",
    "            return\n",
    "\n",
    "        # Samples a batch from the memory\n",
    "        random_batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        state = np.zeros((self.batch_size, self.state_size))\n",
    "        next_state = np.zeros((self.batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            state[i] = random_batch[i][0]\n",
    "            action.append(random_batch[i][1])\n",
    "            reward.append(random_batch[i][2])\n",
    "            next_state[i] = random_batch[i][3]\n",
    "            done.append(random_batch[i][4])\n",
    "\n",
    "        # Batch prediction to save compute costs\n",
    "        target = self.model.predict(state)\n",
    "        target_next = self.model(next_state)\n",
    "\n",
    "        for i in range(len(random_batch)):\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.gamma * (np.amax(target_next[i]))\n",
    "\n",
    "        self.model.fit(\n",
    "            np.array(state),\n",
    "            np.array(target),\n",
    "            batch_size=self.batch_size,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def load_weights(self, weights_file):\n",
    "        self.epsilon = self.epsilon_min\n",
    "        self.model.load_weights(weights_file)\n",
    "\n",
    "    def save_weights(self, weights_file):\n",
    "        self.model.save_weights(weights_file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Flag used to enable or disable screen recording\n",
    "    recording_is_enabled = False\n",
    "\n",
    "    # Initializes the environment\n",
    "    env = gym.make('MountainCar-v0')\n",
    "\n",
    "    # Records the environment\n",
    "    if recording_is_enabled:\n",
    "        env = gym.wrappers.Monitor(env, \"recording\", video_callable=lambda episode_id: True, force=True)\n",
    "\n",
    "    # Defines training related constants\n",
    "    num_episodes = 1000\n",
    "    num_episode_steps = env.spec.max_episode_steps  # constant value\n",
    "    action_size = env.action_space.n\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    max_reward = 0\n",
    "\n",
    "    # Creates the agent\n",
    "    agent = Agent(state_size=state_size, action_size=action_size)\n",
    "\n",
    "    # Loads the weights\n",
    "    if os.path.isfile(\"mountain-car-v0.h5\"):\n",
    "        agent.load_weights(\"mountain-car-v0.h5\")\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # Defines the total reward per episode\n",
    "        total_reward = 0\n",
    "\n",
    "        # Resets the environment\n",
    "        observation = env.reset()\n",
    "\n",
    "        # Gets the state\n",
    "        state = np.reshape(observation, [1, state_size])\n",
    "\n",
    "        for episode_step in range(num_episode_steps):\n",
    "            # Renders the screen after new environment observation\n",
    "            #env.render(mode=\"human\")\n",
    "\n",
    "            # Gets a new action\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # Takes action and calculates the total reward\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Recalculates the reward\n",
    "            if observation[1] > state[0][1] >= 0 and observation[1] >= 0:\n",
    "                reward = 20\n",
    "            if observation[1] < state[0][1] <= 0 and observation[1] <= 0:\n",
    "                reward = 20\n",
    "            if done and episode_step < num_episode_steps - 1:\n",
    "                reward += 10000\n",
    "            else:\n",
    "                reward -= 25\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            # Gets the next state\n",
    "            next_state = np.reshape(observation, [1, state_size])\n",
    "\n",
    "            # Memorizes the experience\n",
    "            agent.memorize(state, action, reward, next_state, done)\n",
    "\n",
    "            # Updates the state\n",
    "            state = next_state\n",
    "\n",
    "            # Updates the network weights\n",
    "            agent.experience_replay()\n",
    "\n",
    "            if done:\n",
    "                print(\"Episode %d/%d finished after %d episode steps with total reward = %f.\"\n",
    "                      % (episode + 1, num_episodes, episode_step + 1, total_reward))\n",
    "                break\n",
    "\n",
    "            elif episode_step >= num_episode_steps - 1:\n",
    "                print(\"Episode %d/%d timed out at %d with total reward = %f.\"\n",
    "                      % (episode + 1, num_episodes, episode_step + 1, total_reward))\n",
    "\n",
    "        # Saves the weights\n",
    "        if total_reward >= max_reward:\n",
    "            agent.save_weights(\"mountain-car-v0.h5\")\n",
    "            max_reward = total_reward\n",
    "\n",
    "    # Closes the environment\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7wyEEe7gdzt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "rl.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
